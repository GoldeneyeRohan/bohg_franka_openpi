{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "\n",
    "from openpi.models import model as _model\n",
    "from openpi.policies import droid_policy\n",
    "from openpi.policies import policy_config as _policy_config\n",
    "from openpi.shared import download\n",
    "from openpi.training import config as _config\n",
    "from openpi.training import data_loader as _data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy inference\n",
    "\n",
    "The following example shows how to create a policy from a checkpoint and run inference on a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (10, 8)\n"
     ]
    }
   ],
   "source": [
    "config = _config.get_config(\"pi0_droid\")\n",
    "checkpoint_dir = download.maybe_download(\"s3://openpi-assets/checkpoints/pi0_base\")\n",
    "\n",
    "# Create a trained policy.\n",
    "policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
    "\n",
    "# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\n",
    "example = droid_policy.make_droid_example()\n",
    "result = policy.infer(example)\n",
    "\n",
    "# Delete the policy to free up memory.\n",
    "del policy\n",
    "\n",
    "print(\"Actions shape:\", result[\"actions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a live model\n",
    "\n",
    "\n",
    "The following example shows how to create a live model from a checkpoint and compute training loss. First, we are going to demonstrate how to do it with fake data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# config = _config.get_config(\"pi0_aloha_sim\")\n",
    "\n",
    "# checkpoint_dir = download.maybe_download(\"s3://openpi-assets/checkpoints/pi0_aloha_sim\")\n",
    "config = _config.get_config(\"pi0_droid\")\n",
    "checkpoint_dir = download.maybe_download(\"s3://openpi-assets/checkpoints/pi0_base\")\n",
    "key = jax.random.key(0)\n",
    "\n",
    "# Create a model from the checkpoint.\n",
    "model = config.model.load(_model.restore_params(checkpoint_dir / \"params\"))\n",
    "\n",
    "# We can create fake observations and actions to test the model.\n",
    "obs, act = config.model.fake_obs(), config.model.fake_act()\n",
    "\n",
    "# Sample actions from the model.\n",
    "loss = model.compute_loss(key, obs, act)\n",
    "print(\"Loss shape:\", loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create a data loader and use a real batch of training data to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Repo ID is not set. Cannot create dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m config = dataclasses.replace(config, batch_size=\u001b[32m2\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a single batch of data. This is the same data that will be used during training.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# NOTE: In order to make this example self-contained, we are skipping the normalization step\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# since it requires the normalization statistics to be generated using `compute_norm_stats`.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loader = \u001b[43m_data_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_norm_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m obs, act = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Sample actions from the model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/data_attribution/bohg_franka_openpi/src/openpi/training/data_loader.py:155\u001b[39m, in \u001b[36mcreate_data_loader\u001b[39m\u001b[34m(config, sharding, skip_norm_stats, shuffle, num_batches, num_workers)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a data loader for training.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m    141\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m        execute in the main process.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m data_config = config.data.create(config.assets_dirs, config.model)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m dataset = \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m dataset = transform_dataset(dataset, data_config, skip_norm_stats=skip_norm_stats)\n\u001b[32m    158\u001b[39m data_loader = TorchDataLoader(\n\u001b[32m    159\u001b[39m     dataset,\n\u001b[32m    160\u001b[39m     local_batch_size=config.batch_size // jax.process_count(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     seed=config.seed,\n\u001b[32m    166\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/data_attribution/bohg_franka_openpi/src/openpi/training/data_loader.py:88\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(data_config, model_config)\u001b[39m\n\u001b[32m     86\u001b[39m repo_id = data_config.repo_id\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRepo ID is not set. Cannot create dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id == \u001b[33m\"\u001b[39m\u001b[33mfake\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FakeDataset(model_config, num_samples=\u001b[32m1024\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Repo ID is not set. Cannot create dataset."
     ]
    }
   ],
   "source": [
    "# Reduce the batch size to reduce memory usage.\n",
    "config = dataclasses.replace(config, batch_size=2)\n",
    "\n",
    "# Load a single batch of data. This is the same data that will be used during training.\n",
    "# NOTE: In order to make this example self-contained, we are skipping the normalization step\n",
    "# since it requires the normalization statistics to be generated using `compute_norm_stats`.\n",
    "loader = _data_loader.create_data_loader(config, num_batches=1, skip_norm_stats=True)\n",
    "obs, act = next(iter(loader))\n",
    "\n",
    "# Sample actions from the model.\n",
    "loss = model.compute_loss(key, obs, act)\n",
    "\n",
    "# Delete the model to free up memory.\n",
    "del model\n",
    "\n",
    "print(\"Loss shape:\", loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi0ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
